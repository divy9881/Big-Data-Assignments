1. There was no pretrained model for you token to me.
2. perf stat command errors out when we pass large sentences as part of the command (>16k). Therefore, planning to write to a file and then read it in tokenizer script, but it adds overhead to stats.
